<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0080)http://www.ccs.neu.edu/home/vip/teach/MLcourse/1_intro_DT_RULES_REG/hw1/hw1.html -->
<html dir="ltr"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>HW1</title>
    <meta name="CREATED" content="20050506;19503200">
    <meta name="CHANGED" content="20060531;23041200">
  </head>
  <body>
    <h1><font face="Luxi Sans">CS6140 Machine Learning</font></h1>
    <h1><font face="Luxi Sans">HW1 Decision Tree, Linear Regression<br>
      </font></h1>
    <p>Make sure you check the <a href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/html/schedulen.html"><font face="Luxi Sans">syllabus</font></a> for the due date. Please
      use the notations adopted in class, even if the problem is stated
      in the book using a different notation.</p>
    <p>We are not looking for very long answers (if you find yourself
      writing more than one or two pages of typed text per problem, you
      are probably on the wrong track). Try to be concise; also keep in
      mind that good ideas and explanations matter more than exact
      details.</p>
    <p>Submit all code files Dropbox (create folder HW1 or similar
      name). Results can be pdf or txt files, including plots/tabels if
      any.<br>
    </p>
    <p>"Paper" exercises: submit using Dropbox as pdf, either typed or
      scanned handwritten.<br>
    </p>
    <hr>
    <h3 style="font-weight: normal;">DATASET 1<span style="font-weight:
        bold;">:</span><small> Housing data, <a href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/data/housing_train.txt">training</a> and <a href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/data/housing_test.txt">testing</a> sets (<a href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/data/housing_desc.txt">description</a>). The last
        column are the labels</small>. <br>
    </h3>
    <p>DATATSET 2<span style="font-weight: bold;">: </span> <a href="http://archive.ics.uci.edu/ml/datasets/Spambase">Spambase</a>
      dataset available from the <a href="http://archive.ics.uci.edu/ml/index.html">UCI Machine
        Learning Repository</a>.</p>
    <p>You can try to normalize each column (feature) separately with
      wither one of the following ideas. Do not normalize labels.<span style="font-weight: bold;"></span><br>
    </p>
    <ul>
      <li>Shift-and-scale normalization: substract the minimum, then
        divide by new maximum. Now all values are between 0-1</li>
      <li>Zero mean, unit variance : substract the mean, divide by the
        appropriate value to get variance=1.</li>
      <li>When normalizing a column (feature), make sure to normalize
        its values across all datapoints (train,test,validation, etc)</li>
    </ul>
    <p>Use K-fold cross-validation : <br>
    </p>
    <ul>
      <li> split into K folds </li>
      <li> run your algorithm K times each time training on K-1 of them
        and testing on the remaining one</li>
      <li> average the error across the K runs.</li>
    </ul>
    <h3>PROBLEM 1 [50 points] <br>
    </h3>
    <p>Using each dataset, build a decision tree (or regression tree)
      from the training set. Since the features are numeric values, you
      will need to use thresholds mechanisms. Report (txt or pdf file)
      for each dataset the training and testing error for each of your
      trials:</p>
    <ul>
      <li>simple decision tree using something like Information Gain or
        other Entropy-like notion of randomness</li>
      <li>regression tree</li>
      <li>try to limit the size of the tree to get comparable training
        and testing errors (avoid overfitting typical of deep trees)<br>
      </li>
    </ul>
    <br>
    <h3>PROBLEM 2 [50 points]</h3>
    <p>Using each of the two datasets above, apply regression on the
      training set to find a linear fit with the labels.&nbsp; Implement
      linear algebra exact solution (normal equations). <br>
    </p>
    <ul>
      <li>Compare the training and testing errors (sum of square
        differences between prediction and actual label).</li>
      <li>Compare with the decision tree results</li>
      <li>Use a 10-fold cross validation schema.<br>
      </li>
    </ul>
    <p></p>
    <h3><br>
    </h3>
    <h3>PROBLEM 3 [20 points] <br>
    </h3>
    DHS chapter8, Pb1. Given an arbitrary decision tree, it might have
    repeated queries splits (feature f, threshold t) on some paths
    root-leaf. Prove that there exists an equivalent decision tree only
    with distinct splits on each path.<br>
    <p></p>
    <p>DHS chapter8, <br>
      a) Prove that for any arbitrary tree, with possible unequal
      branching ratios throughout, there exists a binary tree that
      implements the same classification functionality.<br>
      <span style="font-weight: bold;"></span>&nbsp;&nbsp;&nbsp; b)
      Consider a tree with just two levels&nbsp; - a root node&nbsp;
      connected to B leaf nodes (B&gt;=2) . What are&nbsp; then upper
      and the lower limits on the number of levels in a functionally
      equivalent binary tree, as a function of B?<br>
      &nbsp;&nbsp;&nbsp; c)&nbsp; As in b), what are the upper and lower
      limits on number of nodes in a functionally equivalent binary
      tree? <br>
      <span style="font-weight: bold;"></span></p>
    <p></p>
    <h3><br>
    </h3>
    <h3>PROBLEM 4 [20 points]</h3>
    DHS chapter8,<br>
    Consider training a binary decision tree using entropy splits.<br>
    &nbsp;&nbsp;&nbsp; a) Prove that the decrease in entropy by a split
    on a binary yes/no feature can never be greater than 1 bit.<br>
    &nbsp;&nbsp;&nbsp; b)&nbsp; Generalize this result to the case&nbsp;
    of arbitrary branching&nbsp; B&gt;1. <br>
    <p></p>
    <br>
    <h3>PROBLEM 5 [20 points]</h3>
    <p>Write down explicit formulas for normal equations solution
      presented in class for the case of one input dimension. <br>
    </p>
    <p>(Essentially assume the data is (x_i,y_i) i=1,2,..., m and you
      are looking for h(x) = ax+b that realizes the minimum mean square
      error. The problem asks you to write down explicit formulas for a
      and b.)</p>
    <p>HINT: Do not simply copy the formulas from <a href="http://mathworld.wolfram.com/LeastSquaresFitting.html">here</a>
      (but do read the article): either take the general formula derived
      in class and make the calculations (inverse, multiplications,
      transpose) for one dimension or derive the formulas for a and b
      from scratch; in either case show the derivations. You can compare
      your end formulas with the ones linked above. </p>
    <span style="font-weight: bold;"><br>
    </span>
    <h3>PROBLEM 6 [Extra Credit, read DHS ch5]</h3>
    DHS chapter5<br>
    A classifier is said to be a piecewise linear machine if its
    discriminant functions have the form<br>
    <img style="width: 263px; height: 60px;" alt="" src="./HW1_files/HW1_2.png"><br>
    where<br>
    <img style="width: 423px; height: 77px;" alt="" src="./HW1_files/HW1_3.png"><br>
    <br>
    (a) Indicate how a piecewise linear machine can be viewed in terms
    of a linear machine for classifying subclasses of patterns.<br>
    (b) Show that the decision regions of a piecewise linear machine can
    be non convex and even multiply connected.<br>
    (c) Sketch a plot of gij(x) for a one-dimensional example in which
    n1 = 2 and n2 = 1 to illustrate your answer to part (b)<br>
    <h3>PROBLEM 7 [20points]</h3>
    DHS chapter5,<br>
    The convex hull of a set of vectors xi,i = 1,...,n is the set of all
    vectors of the form<br>
    <br>
    <div style="text-align: center;"><img style="width: 133px; height:
        68px;" alt="" src="./HW1_files/latex-image-1.jpeg"><br>
    </div>
    <br>
    where the coefficients Î±i are nonnegative and sum to one. Given two
    sets of vectors, show that either they are linearly separable or
    their convex hulls intersect. (Hint: Suppose that both statements
    are true, and consider the classification of a point in the
    intersection of the convex hulls.)<br>
    <br>
    <h3>PROBLEM 8 [ExtraCredit]</h3>
    <p>With the notation used in class (and notes), prove that </p>
    <p><img alt="" src="./HW1_files/delta_trace_pb.jpg" height="32" width="329"> </p>
    <p></p>
    <p></p>
  

</body></html>